= Оцифровка рабочего в режиме реального времени
:revealjs_theme: black
:revealjs_customtheme: theme.css
:revealjs_slideNumber:
:revealjs_history:
:revealjs_progress:
:encoding: UTF-8
:lang: ru
include::_doc_general_attributes.adoc[]
:doctype: article
:toclevels: 3
:imagesdir: images\realtime_worker_digitizing_habr
:source-highlighter: highlightjs
:highlightjsdir: highlight
:icons: font
:iconfont-remote!:
:iconfont-name: font-awesome-4.7.0/css/font-awesome
:revealjs_mouseWheel: true
:revealjs_center: false
:revealjs_transition: none
:revealjs_width: 1600
:revealjs_height: 900

:!figure-caption:

Привет, Хабр! Я Алексей Коняев, ведущий разработчик в КРОК.

Последние пару лет участвую в проекте "Цифровой рабочий" в роли ведущего java-разработчика.
Если в двух словах, то это система, которая позволяет предотвращать внештатные ситуации на производстве
благодаря определению местоположения людей с помощью носимых устройств Outdoor/Indoor-навигации,
т.е. когда навигация и на улице, и внутри зданий.

Представьте, что вы приехали на экскурсию на завод. Там огромная территория и
вы вместе с гидом передвигаетесь на машине, он рассказывает: "посмотрите направо, здесь новое здание литейного цеха,
а вот слева старое здание, которое скоро должны снести...". Как вдруг через минуту это старое здание взрывают!
Гид, конечно, в шоке, да и вы тоже, но, к счастью, все обошлось:) Спрашивается,
какого черта машина с экскурсантами оказалась в месте проведения взрывных работ?!
И наша система на этот вопрос тоже не ответит, но она поможет вовремя предупредить всех заинтересованных
лиц о том, что в геозоне, где сейчас проводятся опасные работы, появились посторонние.

Еще пример: сотрудник залез на стремянку, чтобы затянуть вентиль газовой трубы.
Резьба вдруг сорвалась, сотрудник не удержался и, упав с высоты 5 метров, от удара потерял сознание.
А вентиль при этом открылся и газ начал поступать в помещение.
Но датчики падения и удара, которые встроны в носимое устройство, передали сигналы на сервер.
Там эти два сигнала были обработаны и алгоритм выявления внештатных ситуаций сформировал
событие-тревогу, которое было передано оператору, а также другим сотрудникам,
которые находятся недалеко от пострадавшего и могут быстро прийти ему на помощь.

Ещё "Цифровой рабочий" позволяет строить различную аналитику, в том числе realtime, а также
выполнять "разбор полетов", т.е. воспроизводить историю событий, чтобы можно было выяснить,
что привело к нежелательной ситуации и постараться избежать ее в будущем.

Как именно все это внутри работает и как мы используем Kafka, Esper и Clickhouse, я расскажу под катом.

image::digital_worker.png[title="Пользовательский интерфейс \"Цифрового рабочего""]

== Архитектура
image::architecture_0.jpg[title="Архитектура \"Цифрового рабочего\""]

Когда мы только начинали проектировать "Цифровой рабочий", то решили пойти по пути
https://ru.wikipedia.org/wiki/%D0%A1%D0%BE%D0%B1%D1%8B%D1%82%D0%B8%D0%B9%D0%BD%D0%BE-%D0%BE%D1%80%D0%B8%D0%B5%D0%BD%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D0%B0%D1%8F_%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0[Событийно-ориентированной архитектуры].

Рассуждали мы так: все начинается с носимых устройств, или, как мы их называем "меток".
Эти метки, по сути, просто умеют передавать с определенной частотой какую то телеметрию или, другими словами, информацию
об изменении своего внутреннего состояния. И вот это изменение состояния метки и есть входное для системы событие.
Далее, нам нужно уметь обрабатывать поток этих входных события, причем, это должен быть не просто последовательный конвейер,
а параллельная обработка разными модулями, которые в процессе работы будут порождать новые события,
обрататываемые другими модулями и т.д.
В итоге, какие то события будут передаваться в UI, чтобы отрисовывать перемещение объектов на карте.

В качестве шины или слоя передачи событий мы выбрали https://kafka.apache.org/[Apache Kafka].
На тот момент Kafka уже зарекомендовала себя
как зрелый и надежный продукт (мы начинали с версии 2.0.0). Кроме того, выбирали только среди Open-source решений,
чтобы удовлетворить требованиям импортозамещения. А с функциональной точки зрения, в Kafkа нам понравилась возможность
независимо подключать различных консьюмеров к одному и тому же топику,
возможность прочитать события из топика еще раз за период в прошлом,
механизм стриминговой обработки Kafka Streams, ну и, конечно, масштабируемость благодаря партиционированию топиков.

Архитектура система включает следующие компоненты:

* Есть различные носимые устройства (метки) и системы позиционирования, которые уже существуют на рынке и которые умеют
работать с теми или иными девайсами.
* Для каждой такой системы позиционирования, с которой мы решили интегрироваться, у нас есть свой модуль *Адаптер*, который
публикует в kafka события с телеметрией от меток.
* Далее, эти события обрабатываются *Транслятором*, который выполняет первичную обработку (связывание метки с сотрудником,
вычисление геозоны, в которой находится сейчас метка и др.).
* Модуль Complex Event Processing-а (*CEP-процессор*) обрабатывает события, которые порождает Транслятор;
здесь мы занимаемся выявлением внештатных ситуаций, анализируя различные типы событий, в том числе от разных меток.
* В *UI* поступают собятия как от Транслятора (для отрисовки перемещения сотрудников), так и от CEP-процессора
(отображение алертов).
* Для хранения справочных данных, как то список меток, сотрудников, геозон и пр., используем реляционную БД - *PostgreSQL*.
* А для хранения данных, по которым строится аналитика - *ClickHouse*.
* Но в ClickHouse напрямую никто не ходит - для этого используется модуль Reports, который выполняет обработку
запросов к аналитическим данным (запросы на обновление данных виджетов на аналитической панели в UI,
запросы на формирование различных отчетов и др.).
* И еще есть файловое хранилище *S3*, где мы храним файлы 3D-моделей и файлы сформированных отчетов.

Ну а теперь давайте расскажу поподробнее про все модули системы, как они устроены внутри.

== Адаптеры
Основная задача Адаптера - взаимодействие с системой позиционирования через ее API для того, чтобы получать информацию
о координатах метки и значения встроенных в метку датчиков, например, заряд аккумулятора, статус нажатия тревожной кнопки,
статус датчика падения и неподвижности, температура, владность, уровень CO2 и др.

Системы позиционирования - это не разработка КРОК, а уже существующие на рынке системы, которые, как правило,
умеют взаимодействовать с одним каким то конкретным девайсом.
Мы же, подключая через адаптеры такие системы, получаем возможность использовать в "Цифровом рабочем" большой
ассортимент разных меткок.
Хотя для некоторых меток мы все таки запилили систему позиционирования сами, когда не удавалось
найти существующего решения или хотелось поэкспериментировать.
А один из экспериментов привел к тому, кто КРОК разработал свою метку с креплением на каску:

image::helmet.png[title="Каска с умным модулем КРОК"]

Еще одна важная особенность адаптеров - это сложность с их масштабированием. Когда меток много, то важно успевать
обрабатывать информацию от всех, и решение в лоб - поставить несколько адаптеров.
Но сделать это не всегда просто, т.к. мы можем упереться в ограничения API системы позиционирования,
например, когда адаптер с определенной периодичностью дергает рест-апи и получает статус сразу всех меток.
Поэтому основное требование к адаптеру - быть *максимально производительным*, т.е. получил данные в чужом формате,
преобразовал к нашему внутреннему и отправил событие в Kafka. Эти события, которые порождает адаптер, мы назваем *Сырые*.

Сырые события бывают такие:

* TagMovement - перемещение метки, содержит координаты;
* TagInfo - телеметрия со значениями датчиков метки;
* TagAlert - события нажатия трефожной кнопки, событие падения и удара.

== Топики Kafka

Отдельно хочу остановиться на топиках.
Для каждого типа события в "Цифровом рабочем" используется свой отдельный топик.
Такой подход позволяет:

* Индивидуально настраивать retention для каждого топика. Так, например, для сырых событий ретеншн всего 1 сутки, потому
что эти события практически сразу же будут обработаны и хранить их долго не нужно (но 1 сутки все таки храним на случай
сбоя).
* Использовать надстройку над Kafka, которую мы сделали, чтобы модули, которым нужно читать или писать из Kafka,
могли просто подписываться на события заданного типа, или просто публиковать события, не задумываясь о топике, который
определяется автоматически.
* Реализовать автоматическую сборку топологии Kafka Streams процессоров - если вкратце, то работает она так:
** в прикладном модуле нужно объявить процессоры, указав тип "входного" события и тип "выходного";
** также можно указать, будет ли данный процессор использовать состояние;
** все процесоры - это Spring Bean-ы;
** при запуске приложения сборщик топологии находит все процессоры и собирает их в граф, "стыкуя" процесоры так,
чтобы тип "выхода" одного подходил под тип "входа" другого.

Примерно так выглядит топология процессоров Транслятора, о котором речь пойдет ниже:

image::topology.jpg[title="Топология процессоров Транслятора"]

== Транслятор

Транслятор - это модуль, который выполняет обработку "сырых" событий, поступающих от адаптеров. При этом, какой конкретно
адаптер был источником события, на этом этапе обработки уже не важно. Все "сырые" события одинаковые.
Это, кстати, позволило реализовать адаптер-заглушку, который мы используем для отладки и тестирования.
С его помощью можно управлять перемещением сотрудников клавишами на клавиатуре, чтобы не бегать каждый раз по офису
с реальной меткой не очень прикольно, хотя с точки зрения физкультуры - это полезно:)

Внутри Транслятора несколько процессоров на Kafka Streams
(см. https://kafka.apache.org/27/documentation/streams/developer-guide/processor-api.html[processor-api]), причем многие из них используют состояние.
Kafka Streams предоставляет API для работы с состоянием, как с Key-Value таблицей. Тут важно понимать, что для каждого
ключа входного события процессора существует свое состояние. Ключ у каждого типа события свой, например,
для события перемещения метки это будет серийный номер метки. Это позволяет выполнять обработку очередного события
от какой то конкретной метки с учетом истории обработки событий от этой же метки.

Транслятор решает следующие задачи:

* Вычисление географических координат. Дело в том, что координаты, которые приходят в сырых событиях, могут содержать
не широту и долготу, а, например, смещение по оси X и Y в метрах, относительно внутренней системы координат
системы позиционирования. И, помня о том, что Адаптеры должны работать максимально быстро, приведением координат
к абсолютным или географическим занимается Транслятор.
* Связывание метки и сотрудника. Здесь процессор Транслятора обращается к реляционной БД за справочной информацией, чтобы
определить, какой именно сотрудник сейчас носит эту метку.
* Определение остановки сотрудника. Здесь мы смотрим, если новые координаты метки не сильно отличаются от предыдущих
в течение некоторого таймаута, то значит сотрудник остановился.
* Обнаружение потери сигнала от метки. Тут используем такую штуку как Punctuator - это механизм Kafka Streams, который
позволяет с заданной периодичностью просматривать состояния процессора по всем ключам. И логика простая: если нашли состояние,
в котором время последних полученных координат позже, чем максимально разрешенное, то значит сигнала от метки не было
давно и следует считать сигнал потерянным.
* Еще есть процессоры, которые работают с геозонами. Суть в том, что вся территория и здания размечаются на геозоны.
Например, "Корпус Альфа", который в свою очередь разделяется на геозоны этажей, а каждый этаж - на корридоры и кабинеты.
И есть процессор, который определеяет, что координаты метки попали внутрь той или иной геозоны, а другой процессор -
выполняет подсчет количества сотрудников внутри геозон.

События, которые создаются в результате работы Транслятора, мы называем бизнес-события, потому что:

* эти события уже представляют интерес для пользователей системы - события перемещения сотрудников передаются в UI
для отрисовки их на карте; также в UI отображается телеметрия метки, которая уже ассоциирована с определенным сотрудником;
* почти все бизнес-события сохраняются в аналитическую БД;
* многие бизнес-события используются для выявления внештатных ситуаций;
* и еще эти события мы храним в Kafka долго (1 месяц) для того, чтобы иметь возможность воспроизвести их и посмотреть,
что происходило в определенный интервал времени в прошлом.

=== Кэширование при потоковой обработке

Когда мы начали проводить нагрузочное тестирование, то оказалось, что Транслятор сильно тормозит, и связано это было
с тем, что многие его процессоры при обработке каждого события ходили в БД за справочной информацией.
Естесственным решением проблемы было кэширование доступа к БД.
Но кэш нужен был не совсем уж простой, т.к. информация в справочниках хоть и меняется редко, но все равно это может
произойти в процессе работы. Например, сотрудник потерял метку и ему выдали другую.
И, кроме того, некоторые процессоры в результате работы могут обновлять какие то данные в БД.
Поэтому нужен был кэш, который будет:

* обновляться с некоторой периодичностью;
* периодически "сливать" изменения в БД.

Также нужно было иметь в виду, что Трансляторы мы точно будет запускать в несколько экземпляров, чтобы масштабировать
на нагрузку. И при этом всем нодам Транслятора должен быть доступен одинаковый кэш. Т.е. запаздание при чтении
справочников из БД не страшны, но если в ходе обработки сообщений какой то процессор меняет значение в кэше,
то это новое значение должны быть сразу же доступно всем нодам Транслятора.

Мы сделали выбор в пользу https://hazelcast.com[Hazelcast]-а, потому что:

* Его достаточно легко использовать - это просто библиотека, которую вы полключаете в проект.
* Кэши Hazelcast-а автоматически синхронизируются на всех нодах. Но тут нужно быть осторожным - если не ограничить
"область видимости" кэша через задание имени группы (в конфиге экземпляра Hazelcast-а),
то он может незаметно для вас реплицироваться в каком-нибудь еще модуле, где вы тоже решили использовать Hazelcast.
Т.е. в нашем случае, все кэши, которые нужны Трансляторам и только им, объединены в группу "translator".

После добавления кэширования производительность Транслятора выросла на несколько порядков!
Цифры получились такие: одна нода, которой выделены ресурсы, эквивалентные инстансу t4g.micro
в облаке Amazon EC2 (2CPU + 2Gb), обрабатывала без задержки до 500 входящих "сырых" событий в секунду.
500 кажется не много, но метки разных производителей передают данные с разной частотой --
от 3 событий в минуту до 5 событий в секунду. И высокопроизводительные метки, которые дают большую точность,
могут использоваться не на всей территории объекта. Таким образом, в худшем случае одна нода Транслятора выдерживает
нагрузку от 100 меток, а в лучшем - от 10 тысяч.

== Отображение объектов на карте

UI состоит из двух частей - серверная часть и клиент.
Серверная часть подписывается на определенные бизнес-события, в первую очередь -- события перемещения сотрудников.
Эти события через WebSocket передаются на клиента, но предварительно выполняется:

* фильтрация - каждому клиенту отправляются только те события, которые относятся к текущей выбранной оператором геозоне,
т.е. если оператор выбрал, например, 7 этаж, то клиент будет получать события только по этому этажу;
* редукция событий - события накапливаются в буфере и отправляет на клиента 1 раз в секунду.

Это нужно для того, чтобы снизить нагрузку на клиента.

Клиентская часть - это web-приложение на React-е. Основную рабочую область UI занимает карта с 3D-моделями зданий,
которые можно посмотреть "в разрезе" - провалиться на любой этаж и увидеть, что там происходит.
Для отрисовки 3D-моделей мы используем библиотеку https://cesium.com/cesiumjs[CesiumJS].

== Complex Event Processing

image::cep.png[]

Термин "Complex Event Processing" (CEP) придумал профессор стендфордского университета
https://en.wikipedia.org/wiki/David_Luckham[David Luckham].
Вкратце, определение звучит так: "Complex Event Processing - это обработка потока различных событий
в реальном времени с целью выявления паттернов значимых событий".

Еще часто CEP сравнивают с ESP (Event Stream Processing). И здесь David Luckham выделяет следующие отличия между ними:

ESP -- это обработка упорядоченного потока события одного типа, которая выполняется, как правило, с целью фильтрации,
трансформации событий и/или выполнения над потоком событий каких либо математических операций, в том числе, агрегация и
группировка.

CEP -- это обработка нескольких потоков различных событий, причем, необязательно упорядоченных (т.е. допускается нарушение
порядка поступления событий в сравнении с порядком их возникновения), которая выполняется внутри некоторого "окна"
(временного, например) с учетом причинно-следственной связи между различными событиями.

Например, обработка событий от термометра с целью управления кондиционером
(температура стала ниже порога - включили обогрев, и наоборот) - это ESP.
А вот обработка событий от термометра и одновременно от датчика освещенности в течение суток позволяет сделать вывод,
что на улице зима.

Но, в тоже время, в одной из своих статей David Luckham с коллегой видя, что современные
инструменты Event Stream Processing-а все больше и больше приобретают возможности CEP-инструментов,
делают вывод, что со временем разница между ними будет стерта (см.
https://complexevents.com/2020/06/17/the-future-of-event-stream-analytics-and-cep[The Future of Event Stream Analytics and CEP]).

=== CEP-процессор

Давайте перейдем от теории к практике!
Как вы уже поняли, в "Цифровом рабочем" именно модуль CEP-процессор выполняет сложную обработку событий, которыми
в контексте нашей системы являются внештатные ситуации, такие как:

* Вход сотрудника в геозону, имеющую в данный момент статус "опасная".
* Пульс выше или ниже нормы.
* Срабатывание двух из трех датчиков: падение/удар/неподвижность (заставлять реагировать оператора на единичный сигнал
падения неправильно, т.к. метка просто могла упасть, но, если в течение небольшого промежутка времени пришло сразу несколько
таких сигналов, то регистрируется внештатная ситуация).

В качестве CEP-движка мы используем Open-source библеотеку Esper, которую с 2006 года разрабатывает компания
https://www.espertech.com[EsperTech].
Esper мы выбрали по следующим соображениям:

* Описание паттернов сложных событий (или "правил") выполняется на языке Event Processing Language (EPL), который является
расширением стандарта SQL-92; соответственно, правила описываются в декларативном виде, а нам очень хотелось,
чтобы эти правила могли понимать не только программисты, но и, например, аналитики (хотя, если правило действительно
сложное, то без подготовки его будет трудно понять).
* Esper - мощный инструмент и достаточно сложные паттерны можно описать в несколько строк на EPL.
* Есть интеграция с Kafka, которая позволяет описывать правила, оперируя событиями, потребляемыми из Kafka, а результат
работы правила также оформлять в виде события и публиковать в Kafka.

=== Пример реализации правила на Esper-е

Давайте рассмотрим небольшую задачу и ее решение на Esper-е.

В КРОКе есть свой ЦОД и газотурбинные генераторы, которые используются как резервный источник питания.
Эти штуки нужно периодически запускать, но оставлять их включенными без присмотра надолго нельзя.
Предположим, что сотрудник, который их обслуживает не всегда соблюдает регламент и может отлучиться на время, больше
чем разрешенное.

Задача: если оборудование включено и в помещении, где оно расположено, нет ни одного сотрудника больше 10 минут,
то необходимо послать сформировать событие-тревогу.

Для отладки решения будем использовать https://www.esperonline.net/[Esper Notebook], в котором можно описать правило
на EPL-е и сценарий с входными данными.

В правиле сначала нужно определить схемы или таблицы, строки которых будут представлять входные события,
в потоке которых мы будем искать интересующий нас паттерн:
[source,sql92]
----
@Description('Кол-во людей в геозоне')
create schema PersonsInZone(zoneId string, number int);

@Description('Статус устройства (вкл / выкл)')
create schema DeviceStatus(deviceId string, zoneId string, turnedOn bool);
----

Вообще, когда настроена интеграция с Kafka, то определять схемы не обязательно - можно использовать класс события,
как схему. Но для этого, класс события нужно зарегистрировать при инициализации движка Esper-а:

[source,java]
----
import com.espertech.esper.client.Configuration;
import com.espertech.esperio.kafka.EsperIOKafkaConfig;
import java.util.Properties;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.producer.ProducerConfig;

public class MyEsperConfiguration extends Configuration {
    public MyEsperConfiguration() {
        Properties props = new Properties();
        // параметры для Kafka-консюмера
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka:9092");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        // параметры для Kafka-продюсера
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka:9092");
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

        // используем свою реализацию подписчика на топики для того,
        // чтобы подписаться только на нужные нам топики
        props.put(EsperIOKafkaConfig.INPUT_SUBSCRIBER_CONFIG, MyEsperIOKafkaInputSubscriber.class.getName());
        // процессор входных событий - используем реализацию по умолчанию
        props.put(EsperIOKafkaConfig.INPUT_PROCESSOR_CONFIG, EsperIOKafkaInputProcessorDefault.class.getName());

        // используем свою реализацию контроллера выходного потока событий
        // (это события, которые будут создаваться при срабатывании правила),
        // чтобы явно задавать топик, в который нужно опубликовать выходное событие
        props.put(EsperIOKafkaConfig.OUTPUT_FLOWCONTROLLER_CONFIG, MyEsperIOKafkaOutputFlowController.class.getName());

        // регистрируем плагины для интеграции с Kafka
        addPluginLoader("KafkaInput", EsperIOKafkaInputAdapterPlugin.class.getName(), props, null);
        addPluginLoader("KafkaOutput", EsperIOKafkaOutputAdapterPlugin.class.getName(), props, null);

        // регистрируем типы событий
        // в EPL они будут доступны по имени MyEvent.class.getSimpleName()
        addEventType(PersonsInZone.class);
        addEventType(DeviceStatus.class);
    }
}
----

Далее, нас интересует, что происходит, когда оборудование включено, поэтому определим контекст, который "откроется"
при включении оборудования, и "закроется" при его выключении, при этом, для каждой единицы оборудования будет свой
собственный контекст:
[source,sql92]
----
create context TurnerOnDeviceContext
    partition by deviceId from DeviceStatus
    initiated by DeviceStatus(turnedOn = true)
    terminated by DeviceStatus(turnedOn = false);
----
Контекст - это своего рода "окно", в рамках которого мы будет наблюдать входящие события.

[source,sql92]
----
// для выражения можно задать имя, чтобы видеть его в логах
@Name('Unattended device')
// эта аннотация - наша кастомная, ее использует MyEsperIOKafkaOutputFlowController,
// заданный в конфиге выше, чтобы выбрать нужный топик,
// в который будет опубликовано выходное событие
@KafkaOutputTopic(publishTo = 'notification')
// указывая контекст мы говорим, чтобы данное выражение
// выполнялось только для событий внутри контекста
context TurnerOnDeviceContext
select
    // createAlertNotification - это наш java-метод,
    // который создает событие NotificationEvent
    // а transpose - встроенная функция Esper-а, которую нужно использовать
    // в случае, если результатом select-а является java-объект
    transpose(createAlertNotification(
        ds.zoneId,
        ds.deviceId
    ))
from
    // в качестве "источника" событий адаем шаблон,
    // который словами можно сформулировать так:
    // "Устройство включили" → потом "Все ушли" → потом ("Прошло 10 минут" И "Никто не вернулся")
    pattern [
        ds = DeviceStatus(turnedOn = true)
        -> every (
            PersonsInZone(zoneId = ds.zoneId and number = 0)
            -> (timer:interval(10 minutes)
                    and not PersonsInZone(zoneId = ds.zoneId and number > 0)
               )
        )
    ];
----
Здесь еще используется ключевое слово "every", которое нужно для того, чтобы следующая за ним часть шаблона
продолжала "находиться" даже после того, как один раз целый шаблон уже был найден.
Например, если сотрудник включил генератор, потом ушел и прошло больше 10 минут - сработает правило и мы пошлем
нотификацию оператору, тот позвонит сотруднику, отругает его:), и тот вернется.
Но генератор сотрудник так и не выключит, поработает с ним ещё какое то время и, опять все забыв,
снова уйдет больше, чем на 10 минут.
Таким образом, мы получим ситуацию, когда часть шаблона `"Все ушли" → ("Прошло 10 минут" И "Никто не вернулся")`
будет найден снова, в то время, как первая часть `"Устройство включили"` уже была найдена ранее.
И, чтобы целый шаблон найти еще раз, нужно написать `every` перед второй его частью.

Чтобы Esper Notebook понял, что текст - это правило, в самом начале нужно использовать ключевое слово `%esperepl`.
Полный текст правила, который можно запустить в Notebook-е:
[source,sql92]
----
%esperepl
create schema PersonsInZone(zoneId string, number int);
create schema DeviceStatus(deviceId string, zoneId string, turnedOn bool);

create context TurnerOnDeviceContext
    partition by deviceId from DeviceStatus
    initiated by DeviceStatus(turnedOn = true)
    terminated by DeviceStatus(turnedOn = false);

@Name('Unattended device')
context TurnerOnDeviceContext
select
    ds.zoneId, ds.deviceId
from
    pattern [
        ds = DeviceStatus(turnedOn = true)
        -> every (
            PersonsInZone(zoneId = ds.zoneId and number = 0)
            -> (timer:interval(10 minutes)
                    and not PersonsInZone(zoneId = ds.zoneId and number > 0)
               )
        )
    ];
----

Теперь нам нужно протестировать наше правило. Для этого используется сценарий (начинается с ключевого слова `%esperscenario`),
в котором можно "публиковать" входные события, увеличивая текущее время сценария:
[source,sql92]
----
%esperscenario
// задаем начальное время
t = "2020-12-10 12:00:00.000"

// публикуем событие "в помещение A вошел 1 сотрудник"
PersonsInZone = {zoneId="room-A", number=1}

// через 1 минуту включаем генератор 1 в помещении А
t = t.plus(1 minute)
DeviceStatus = {deviceId="generator-1", zoneId="room-A", turnedOn=true}

// через 4 часа сотрудник вышел из помещения А, не выключив генератор
t = t.plus(4 hours)
PersonsInZone = {zoneId="room-A", number=0}

// прошло ещё 10 минут - и должно сработать наше правило!
t = t.plus(10 minute)
----
Если в правиле сработает какое то выражение, то в блоке сценария мы должны увидеть его вывод.
В нашем случае выражение - это select, а вывод будет такой:

`Unattended device-output={ds.zoneId='room-A', ds.deviceId='generator-1'}`

Вот еще один пример сценария посложнее:
[source,sql92]
----
%esperscenario
t = "2020-12-10 12:00:00.000"

// сотрудник вошел в помещение А
PersonsInZone = {zoneId="room-A", number=1}

// через 1 минуту включил генератор 1
t = t.plus(1 minute)
DeviceStatus = {deviceId="generator-1", zoneId="room-A", turnedOn=true}

// через 30 минут вышел, не выключив генератор
t = t.plus(30 minutes)
PersonsInZone = {zoneId="room-A", number=0}

// но через 5 минут вернулся
t = t.plus(5 minute)
PersonsInZone = {zoneId="room-A", number=1}

// прошле еще 5 минут - тревоги не должно быть, сотрудник ведь вернулся
t = t.plus(5 minute)

// еще через 3 часа ушел, а генератор все также остался включенным
t = t.plus(3 hour)
PersonsInZone = {zoneId="room-A", number=0}

// через 10 минут - тревога!
t = t.plus(10 minute)
----

=== Регистрация внештатных ситуаций

CEP-процессор при срабатывании правил выполняет:

* Регистрирует внештатную ситуацию в журнале событий. Мы предполагаем, что события в этот журнал не должны попадать часто,
но если уж попали, то требуется их явная обработка оператором (написать резолюцию, закрыть событие).
* Формирует событие-нотификацию, которое, как и все остальные события в "Цифровом рабочем", публикуются
в свой топик в Kafka. На этот топик подписан модуль **Нотификаций**, который выполняет их маршрутизацию.
В результате нотификация передается в UI (оператор видит уведомление) или конкретному сотруднику через
тот канал, который для него определен (например, на почту, в telegram или на метку, которую носит сотрудник,
если она поддерживает "обратный" канал).

== Аналитика

image::analitics_dashboard.png[title="Аналитическая панель"]

Аналитика доступна оператору в виде панели инструментов, на которую он может вынести интересующие его виджеты,
и в виде механизма формирования отчетов.

Виджеты, по сути, это теже отчеты, у которых есть какие то свои входные параметры,
но результат выводится не в Excel или PDF-файл, а в виде графика или диаграммы.
Еще виджеты обновляются раз в 15 секунд, что позволяет оператору видеть актуальные на текущий момент времени данные.

Также существуют "отчеты" со специализированным отображением информации непосредственно на карте:

image::analitics_heatmap.png[title="Тепловая карта: отображает наиболее часто посещаемые места"]

image::analitics_route.png[title="Маршрут: отображает маршрут передвижения сотрудника"]

Данные, по которым строится аналитика, хранятся в БД ClickHouse. Но прежде, чем попасть в ClickHouse, эти данные
проходят дополнительную обработку в модуле "Подготовки данных". А, чтобы достать эти данные из БД
и передать потребителю (в данном случае, в UI), используется модуль "Формирования отчетов".

=== ClickHouse

image::clickhouse.png[]

К аналитической БД у нас были следующие требования:

* нужно уметь хранить очень много данных -- наши данные, это события, у которых есть ключ и временная метка;
* нужно быстро выполнять запросы с условием "С - ПО", т.е. за определенный период времени.

Когда мы выбирали БД, то понимали, что здесь точно нужна не реляционка, а скорее NoSql, которая умеет линейно
масштабироваться и обеспечивает отказоустойчивость.
На момент проектирования "Цифрового рабочего" у нас был опыт применения NoSql БД Cassandra, но ClickHouse был на слуху
и, поизучав документацию и посмотрев несколько презентаций от ребят, которые уже успешно используют его в проде,
мы тоже решили попробовать.

Из особенностей ClickHouse мы для себя выделили следующие:

* Это колоночная БД -- наши события имеют много атрибутов, которые удобно разложить по колонкам.
* Хорошо подходит для Time-serias данных -- ClickHouse может партиционировать таблицу по ключу, который является
функцией от временной метки события, что позволяет хранить в общей партиции все события полученные, например,
за один и тот же день. А это позволяет эффективно выполнять запросы, у которых задан период времени (сперва будут
выбраны только нужные партиции, а уже потом только по ним выполняется остальная часть запроса).
* Есть встроенная интеграция с Kafka -- ClickHouse напрямую из Kafka умеет загружать данные в таблицы, при этом можно
дополнительно выполнять обработку данных непосредственно в момент их загрузки.
* Есть возможность подключить внешнюю реляционную БД -- мы подключаем справочники, которые храним в PostgreSql, после чего
их можно использовать как обычные таблицы в Sql-выражениях.
* Богатый набор встроенных аналитических функций - например, для формирования "Тепловой карты" мы используем функцию
Квантиль. А еще мне очень нравится работа с массивами в ClickHouse -- можно джойнить элементы колонки-массива одной
таблицы со строками другой таблицы или трансформировать строки таблицы в колонку-массив.
* Масштабируемая и отказоустойчивая -- чем больше нод базы поднимете, тем больше данных можно будет хранить. Плюс, есть
встроенный механизм сжатия данных.

=== Подготовка данных

Модуль подготовки данных подписывается на топики бизнес-событий, которые представляют интерес с точки зрения
аналитики, и выполняет следующие действия:

* фильтрация - бизнес-событий может быть очень много и для некоторых

* задачи модуля
* результат - данные в аналитической БД


=== Формирование отчетов

* запросы через Kafka
* выполнения запросов к ClickHouse-у
* обработка и форматирование результата
* сохранение результата в S3 (если нужно)
* отправка ответа на запрос тоже через Kafka
** немного про то, почему именно через Kafka

== Воспроизведение истории

* Вычитывание событий из Kafka за период в прошлом
* UI - плеер с возможностью указать период и изменять скорость воспроизведения
* Сохранение темпа воспроизведения
* Плюсы и минусы использования Kafka для этой задачи


=== Поток обработки событий

image::flow.jpg[]
* далее буду рассказывать про модули, которые участвуют в обработки событий, начиная от устройств и заканчивая UI


== А что у вас?

На этом буду закругляться. Надеюсь, вам было интересно узнать о том,
как устроен внутри продукт "Цифровой рабочий", который мы разрабатываем в КРОКе.
Но "Цифровой рабочий", на самом деле, только начинает делать первые шаги на своем пути, и активно развивается.
Мы посточнно допиливаем новые фичи, подключаем новые вендорские системы, что-то мы переделываем и оптимизируем.
Иногда экспериментируем с другими технологиями потоковой обработки, но не вместо Kafka, она вне конкуренции:)
Подумываем о том, чтобы прикрутить Machine Learning. В общем, куча идей и планов на будущее!
Было бы интересно услышать в комментариях, разрабатываете ли вы что-то подобное, например,
в области IoT с потоковой обработкой данных, и какие технологии применяете и как.
