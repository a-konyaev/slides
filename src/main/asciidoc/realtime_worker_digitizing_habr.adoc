= Оцифровка рабочего в режиме реального времени
:revealjs_theme: black
:revealjs_customtheme: theme.css
:revealjs_slideNumber:
:revealjs_history:
:revealjs_progress:
:encoding: UTF-8
:lang: ru
include::_doc_general_attributes.adoc[]
:doctype: article
:toclevels: 3
:imagesdir: images\realtime_worker_digitizing_habr
:source-highlighter: highlightjs
:highlightjsdir: highlight
:icons: font
:iconfont-remote!:
:iconfont-name: font-awesome-4.7.0/css/font-awesome
:revealjs_mouseWheel: true
:revealjs_center: false
:revealjs_transition: none
:revealjs_width: 1600
:revealjs_height: 900

:!figure-caption:

Привет, Хабр! Я Алексей Коняев, ведущий разработчик в КРОК.

Последние пару лет участвую в проекте "Цифровой рабочий" в роли ведущего java-разработчика.
Если в двух словах, то это система, которая позволяет предотвращать внештатные ситуации на производстве
благодаря определению местоположения людей с помощью носимых устройств Outdoor/Indoor-навигации,
т.е. когда навигация и на улице, и внутри зданий.

Представьте, что вы приехали на экскурсию на завод. Там огромная территория и
вы вместе с гидом передвигаетесь на машине, он рассказывает: "посмотрите направо, здесь новое здание литейного цеха,
а вот слева старое здание, которое скоро должны снести...". Как вдруг через минуту это старое здание взрывают!
Гид, конечно, в шоке, да и вы тоже, но, к счастью, все обошлось:) Спрашивается,
какого черта машина с экскурсантами оказалась в месте проведения взрывных работ?!
И наша система на этот вопрос тоже не ответит, но она поможет вовремя предупредить всех заинтересованных
лиц о том, что в геозоне, где сейчас проводятся опасные работы, появились посторонние.

Еще пример: сотрудник залез на стремянку, чтобы затянуть вентиль газовой трубы.
Резьба вдруг сорвалась, сотрудник не удержался и, упав с высоты 5 метров, от удара потерял сознание.
А вентиль при этом открылся и газ начал поступать в помещение.
Но датчики падения и удара, которые встроны в носимое устройство, передали сигналы на сервер.
Там эти два сигнала были обработаны и алгоритм выявления внештатных ситуаций сформировал
событие-тревогу, которое было передано оператору, а также другим сотрудникам,
которые находятся недалеко от пострадавшего и могут быстро прийти ему на помощь.

Ещё "Цифровой рабочий" позволяет строить различную аналитику, в том числе realtime, а также
выполнять "разбор полетов", т.е. воспроизводить историю событий, чтобы можно было выяснить,
что привело к нежелательной ситуации и постараться избежать ее в будущем.

Как именно все это внутри работает и как мы используем Kafka, Esper и Clickhouse, я расскажу под катом.

image::digital_worker.png[title="Пользовательский интерфейс \"Цифрового рабочего""]

== Архитектура
image::architecture_0.jpg[title="Архитектура \"Цифрового рабочего\""]

Когда мы только начинали проектировать "Цифровой рабочий", то решили пойти по пути
https://ru.wikipedia.org/wiki/%D0%A1%D0%BE%D0%B1%D1%8B%D1%82%D0%B8%D0%B9%D0%BD%D0%BE-%D0%BE%D1%80%D0%B8%D0%B5%D0%BD%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D0%B0%D1%8F_%D0%B0%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0[Событийно-ориентированной архитектуры].

Рассуждали мы так: все начинается с носимых устройств, или, как мы их называем "меток".
Эти метки, по сути, просто умеют передавать с определенной частотой какую то телеметрию или, другими словами, информацию
об изменении своего внутреннего состояния. И вот это изменение состояния метки и есть входное для системы событие.
Далее, нам нужно уметь обрабатывать поток этих входных события, причем, это должен быть не просто последовательный конвейер,
а параллельная обработка разными модулями, которые в процессе работы будут порождать новые события,
обрататываемые другими модулями и т.д.
В итоге, какие то события будут передаваться в UI, чтобы отрисовывать перемещение объектов на карте.

В качестве шины или слоя передачи событий мы выбрали https://kafka.apache.org/[Apache Kafka].
На тот момент Kafka уже зарекомендовала себя
как зрелый и надежный продукт (мы начинали с версии 2.0.0). Кроме того, выбирали только среди Open-source решений,
чтобы удовлетворить требованиям импортозамещения. А с функциональной точки зрения, в Kafkа нам понравилась возможность
независимо подключать различных консьюмеров к одному и тому же топику,
возможность прочитать события из топика еще раз за период в прошлом,
механизм стриминговой обработки Kafka Streams, ну и, конечно, масштабируемость благодаря партиционированию топиков.

Архитектура система включает следующие компоненты:

* Есть различные носимые устройства (метки) и системы позиционирования, которые уже существуют на рынке и которые умеют
работать с теми или иными девайсами.
* Для каждой такой системы позиционирования, с которой мы решили интегрироваться, у нас есть свой модуль *Адаптер*, который
публикует в кафку события с телеметрией от меток.
* Далее, эти события обрабатываются *Транслятором*, который выполняет первичную обработку (связывание метки с сотрудником,
вычисление геозоны, в которой находится сейчас метка и др.).
* Модуль Complex Event Processing-а (*CEP-процессор*) обрабатывает события, которые порождает Транслятор;
здесь мы занимаемся выявлением внештатных ситуаций, анализируя различные типы событий, в том числе от разных меток.
* В *UI* поступают собятия как от Транслятора (для отрисовки перемещения сотрудников), так и от CEP-процессора
(отображение алертов).
* Для хранения справочных данных, как то список меток, сотрудников, геозон и пр., используем реляционную БД - *PostgreSQL*.
* А для хранения данных, по которым строится аналитика - *ClickHouse*.
* Но в ClickHouse напрямую никто не ходит - для этого используется модуль Reports, который выполняет обработку
запросов к аналитическим данным (запросы на обновление данных виджетов на аналитической панели в UI,
запросы на формирование различных отчетов и др.).
* И еще есть файловое хранилище *S3*, где мы храним файлы 3D-моделей и файлы сформированных отчетов.

Ну а теперь давайте расскажу поподробнее про все модули системы, как они устроены внутри.

== Адаптеры
Основная задача Адаптера - взаимодействие с системой позиционирования через ее API для того, чтобы получать информацию
о координатах метки и значения встроенных в метку датчиков, например, заряд аккумулятора, статус нажатия тревожной кнопки,
статус датчика падения и неподвижности, температура, владность, уровень CO2 и др.

Системы позиционирования - это не разработка КРОК, а уже существующие на рынке системы, которые, как правило,
умеют взаимодействовать с одним каким то конкретным девайсом.
Мы же, подключая через адаптеры такие системы, получаем возможность использовать в "Цифровом рабочем" большой
ассортимент разных меткок.
Хотя для некоторых меток мы все таки запилили систему позиционирования сами, когда не удавалось
найти существующего решения или хотелось поэкспериментировать.
А один из экспериментов привел к тому, кто КРОК разработал свою метку с креплением на каску:

image::helmet.png[title="Каска с умным модулем КРОК"]

Еще одна важная особенность адаптеров - это сложность с их масштабированием. Когда меток много, то важно успевать
обрабатывать информацию от всех, и решение в лоб - поставить несколько адаптеров.
Но сделать это не всегда просто, т.к. мы можем упереться в ограничения API системы позиционирования,
например, когда адаптер с определенной периодичностью дергает рест-апи и получает статус сразу всех меток.
Поэтому основное требование к адаптеру - быть *максимально производительным*, т.е. получил данные в чужом формате,
преобразовал к нашему внутреннему и отправил событие в кафку. Эти события, которые порождает адаптер, мы назваем *Сырые*.

Сырые события бывают такие:

* TagMovement - перемещение метки, содержит координаты;
* TagInfo - телеметрия со значениями датчиков метки;
* TagAlert - события нажатия трефожной кнопки, событие падения и удара.

== Топики Kafka

Отдельно хочу остановиться на топиках.
Для каждого типа события в "Цифровом рабочем" используется свой отдельный топик.
Такой подход позволяет:

* Индивидуально настраивать retention для каждого топика. Так, например, для сырых событий ретеншн всего 1 сутки, потому
что эти события практически сразу же будут обработаны и хранить их долго не нужно (но 1 сутки все таки храним на случай
сбоя).
* Использовать надстройку над кафкой, которую мы сделали, чтобы модули, которым нужно читать или писать из кафки,
могли просто подписываться на события заданного типа, или просто публиковать события, не задумываясь о топике, который
определяется автоматически.
* Реализовать автоматическую сборку топологии Kafka Streams процессоров - если вкратце, то работает она так:
** в прикладном модуле нужно объявить процессоры, указав тип "входного" события и тип "выходного";
** также можно указать, будет ли данный процессор использовать состояние;
** все процесоры - это Spring Bean-ы;
** при запуске приложения сборщик топологии находит все процессоры и собирает их в граф, "стыкуя" процесоры так,
чтобы тип "выхода" одного подходил под тип "входа" другого.

Примерно так выглядит топология процессоров Транслятора, о котором речь пойдет ниже:

image::topology.jpg[title="Топология процессоров Транслятора"]

== Транслятор

Транслятор - это модуль, который выполняет обработку "сырых" событий, поступающих от адаптеров. При этом, какой конкретно
адаптер был источником события, на этом этапе обработки уже не важно. Все "сырые" события одинаковые.
Это, кстати, позволило реализовать адаптер-заглушку, который мы используем для отладки и тестирования.
С его помощью можно управлять перемещением сотрудников клавишами на клавиатуре, чтобы не бегать каждый раз по офису
с реальной меткой не очень прикольно, хотя с точки зрения физкультуры - это полезно:)

Внутри Транслятора несколько процессоров на Kafka Streams
(см. https://kafka.apache.org/27/documentation/streams/developer-guide/processor-api.html[processor-api]), причем многие из них используют состояние.
Kafka Streams предоставляет API для работы с состоянием, как с Key-Value таблицей. Тут важно понимать, что для каждого
ключа входного события процессора существует свое состояние. Ключ у каждого типа события свой, например,
для события перемещения метки это будет серийный номер метки. Это позволяет выполнять обработку очередного события
от какой то конкретной метки с учетом истории обработки событий от этой же метки.

Транслятор решает следующие задачи:

* Вычисление географических координат. Дело в том, что координаты, которые приходят в сырых событиях, могут содержать
не широту и долготу, а, например, смещение по оси X и Y в метрах, относительно внутренней системы координат
системы позиционирования. И, помня о том, что Адаптеры должны работать максимально быстро, приведением координат
к абсолютным или географическим занимается Транслятор.
* Связывание метки и сотрудника. Здесь процессор Транслятора обращается к реляционной БД за справочной информацией, чтобы
определить, какой именно сотрудник сейчас носит эту метку.
* Определение остановки сотрудника. Здесь мы смотрим, если новые координаты метки не сильно отличаются от предыдущих
в течение некоторого таймаута, то значит сотрудник остановился.
* Обнаружение потери сигнала от метки. Тут используем такую штуку как Punctuator - это механизм Kafka Streams, который
позволяет с заданной периодичностью просматривать состояния процессора по всем ключам. И логика простая: если нашли состояние,
в котором время последних полученных координат позже, чем максимально разрешенное, то значит сигнала от метки не было
давно и следует считать сигнал потерянным.
* Еще есть процессоры, которые работают с геозонами. Суть в том, что вся территория и здания размечаются на геозоны.
Например, "Корпус Альфа", который в свою очередь разделяется на геозоны этажей, а каждый этаж - на корридоры и кабинеты.
И есть процессор, который определеяет, что координаты метки попали внутрь той или иной геозоны, а другой процессор -
выполняет подсчет количества сотрудников внутри геозон.

События, которые создаются в результате работы Транслятора, мы называем "бизнес-события", потому что:
* эти события уже представляют интерес для пользователей системы - события перемещения сотрудников передаются в UI
для отрисовки их на карте; также в UI отображается телеметрия метки, которая уже ассоциирована с определенным сотрудником;
* почти все "бизнес-события" сохраняются в аналитическую БД;
* многие "бизнес-события" используются для выявления внештатных ситуаций;
* и еще эти события мы храним в кафке долго (1 месяц) для того, чтобы иметь возможность воспроизвести их и посмотреть,
что происходило в определенный интервал времени в прошлом.

=== Кэширование при потоковой обработке

Когда мы начали проводить нагрузочное тестирование, то оказалось, что Транслятор сильно тормозит, и связано это было
с тем, что многие его процессоры при обработке каждого события ходили в БД за справочной информацией.
Естесственным решением проблемы было кэширование доступа к БД.
Но кэш нужен был не совсем уж простой, т.к. информация в справочниках хоть и меняется редко, но все равно это может
произойти в процессе работы. Например, сотрудник потерял метку и ему выдали другую.
И, кроме того, некоторые процессоры в результате работы могут обновлять какие то данные в БД.
Поэтому нужен был кэш, который будет:
* обновляться с некоторой периодичностью;
* периодически "сливать" изменения в БД.

Также нужно было иметь в виду, что Трансляторы мы точно будет запускать в несколько экземпляров, чтобы масштабировать
на нагрузку. И при этом всем нодам Транслятора должен быть доступен одинаковый кэш. Т.е. запаздание при чтении
справочников из БД не страшны, но если в ходе обработки сообщений какой то процессор меняет значение в кэше,
то это новое значение должны быть сразу же доступно всем нодам Транслятора.

Мы сделали выбор в пользу https://hazelcast.com[Hazelcast]-а, потому что:

* Его достаточно легко использовать - это просто библиотека, которую вы полключаете в проект.
* Кэши Hazelcast-а автоматически синхронизируются на всех нодах. Но тут нужно быть осторожным - если не ограничить
"область видимости" кэша через задание имени группы (в конфиге экземпляра Hazelcast-а),
то он может незаметно для вас реплицироваться в каком-нибудь еще модуле, где вы тоже решили использовать Hazelcast.
Т.е. в нашем случае, все кэши, которые нужны Трансляторам и только им, объединены в группу "translator".

После добавления кэширования производительность Транслятора выросла на несколько порядков!
Цифры получились такие: одна нода, которой выделены ресурсы, эквивалентные инстансу t4g.micro
в облаке Amazon EC2 (2CPU + 2Gb), обрабатывала без задержки до 500 входящих "сырых" событий в секунду.
500 кажется не много, но метки разных производителей передают данные с разной частотой --
от 3 событий в минуту до 5 событий в секунду. И высокопроизводительные метки, которые дают большую точность,
могут использоваться не на всей территории объекта. Таким образом, в худшем случае одна нода Транслятора выдерживает
нагрузку от 100 меток, а в лучшем - от 10 тысяч.

== Отображение объектов на карте

UI состоит из двух частей - серверная часть и клиент.
Серверная часть подписывается на определенные "бизнес-события", в первую очередь -- события перемещения сотрудников.
Эти события через WebSocket передаются на клиента, но предварительно выполняется:

* фильтрация - каждому клиенту отправляются только те события, которые относятся к текущей выбранной оператором геозоне,
т.е. если оператор выбрал, например, 7 этаж, то клиент будет получать события только по этому этажу;
* редукция событий - события накапливаются в буфере и отправляет на клиента 1 раз в секунду.

Это нужно для того, чтобы снизить нагрузку на клиента.

Клиентская часть - это web-приложение на React-е. Основную рабочую область UI занимает карта с 3D-моделями зданий,
которые можно посмотреть "в разрезе" - провалиться на любой этаж и увидеть, что там происходит.
Для отрисовки 3D-моделей мы используем библиотеку https://cesium.com/cesiumjs[CesiumJS].

== Complex Event Processing

image::cep.png[]

Термин "Complex Event Processing" (CEP) придумал профессор стендфордского университета
https://en.wikipedia.org/wiki/David_Luckham[David Luckham].
Вкратце, определение звучит так: "Complex Event Processing - это обработка потока различных событий
в реальном времени с целью выявления паттернов значимых событий".

Еще часто CEP сравнивают с ESP (Event Stream Processing). И здесь David Luckham выделяет следующие отличия между ними:

ESP -- это обработка упорядоченного потока события одного типа, которая выполняется, как правило, с целью фильтрации,
трансформации событий и/или выполнения над потоком событий каких либо математических операций, в том числе, агрегация и
группировка.

CEP -- это обработка нескольких потоков различных событий, причем, необязательно упорядоченных (т.е. допускается нарушение
порядка поступления событий в сравнении с порядком их возникновения), которая выполняется внутри некоторого "окна"
(временного, например) с учетом причинно-следственной связи между различными событиями.

Например, обработка событий от термометра с целью управления кондиционером
(температура стала ниже порога - включили обогрев, и наоборот) - это ESP.
А вот обработка событий от термометра и одновременно от датчика освещенности в течение суток позволяет сделать вывод,
что на улице зима.

Но, в тоже время, в одной из своих статей David Luckham с коллегой видя, что современные
инструменты Event Stream Processing-а все больше и больше приобретают возможности CEP-инструментов,
делают вывод, что со временем разница между ними будет стерта (см.
https://complexevents.com/2020/06/17/the-future-of-event-stream-analytics-and-cep[The Future of Event Stream Analytics and CEP]).

=== CEP-процессор

Давайте перейдем от теории к практике!
Как вы уже поняли, в "Цифровом рабочем" именно модуль CEP-процессор выполняет сложную обработку событий, которыми
в контексте нашей системы являются внештатные ситуации, такие как:

* Вход сотрудника в геозону, имеющую в данный момент статус "опасная".
* Пульс выше или ниже нормы.
* Срабатывание двух из трех датчиков: падение/удар/неподвижность (заставлять реагировать оператора на единичный сигнал
падения неправильно, т.к. метка просто могла упасть, но, если в течение небольшого промежутка времени пришло сразу несколько
таких сигналов, то регистрируется внештатная ситуация).

В качестве CEP-движка мы используем Open-source библеотеку Esper, которую с 2006 года разрабатывает компания
https://www.espertech.com[EsperTech].
Esper мы выбрали по следующим соображениям:

* Описание паттернов сложных событий (или "правил") выполняется на языке Event Processing Language (EPL), который является
расширением стандарта SQL-92; соответственно, правила описываются в декларативном виде, а нам очень хотелось,
чтобы эти правила могли понимать не только программисты, но и, например, аналитики (хотя, если правило действительно
сложное, то без подготовки его будет трудно понять).
* Esper - мощный инструмент и достаточно сложные паттерны можно описать в несколько строк на EPL.
* Есть интеграция с Kafka, которая позволяет описывать правила, оперируя событиями, потребляемыми из кафки, а результат
работы правила также оформлять в виде события и публиковать в кафку.

=== Пример реализации правила на Esper-е

* описание задачи
* описание решения (куски кода)
* Esper Notebook где можно этот пример проверить и поиграться с Esper-ом

=== Нотификации

* это отдельный модуль, который занимается маршрутизацией нотификаций
** на почту, в телеграм, на метку и конечно в UI, чтобы оператору показать
* это тоже Kafka Streams приложение

== Аналитика

* есть реалтайм, есть формирование отчетов за период
* архитектура: подготовка данных - складывание из в аналитич. БД - запросы к БД через модуль отчетов...
* как это выглядит в UI - ниже картинки

image::analitics_dashboard.png[]
Аналитическа доска с разными виджетами

image::analitics_route.png[]
Маршрут передвижения за период

image::analitics_heatmap.png[]
Heatmap - показывает где больше времени провел сотрудник

=== Модуль подготовки данных

* задачи модуля
* результат - данные в аналитической БД

=== ClickHouse
image::clickhouse.png[]
* какие были требования к аналитической БД
* почему выбрали ClickHouse
* как используем ClickHouse:
** загрузка напрямую из Kafka
** подключение справочников из PostgreSql
** аналитические функции

=== Формирование отчетов

* запросы через кафку
* выполнения запросов к ClickHouse-у
* обработка и форматирование результата
* сохранение результата в S3 (если нужно)
* отправка ответа на запрос тоже через Kafka
** немного про то, почему именно через Kafka

== Воспроизведение истории

* Вычитывание событий из Kafka за период в прошлом
* UI - плеер с возможностью указать период и изменять скорость воспроизведения
* Сохранение темпа воспроизведения
* Плюсы и минусы использования кафки для этой задачи


=== Поток обработки событий

image::flow.jpg[]
* далее буду рассказывать про модули, которые участвуют в обработки событий, начиная от устройств и заканчивая UI


== А что у вас?

На этом буду закругляться. Надеюсь, вам было интересно узнать о том,
как устроен внутри продукт "Цифровой рабочий", который мы разрабатываем в КРОКе.
Но "Цифровой рабочий", на самом деле, только начинает делать первые шаги на своем пути, и активно развивается.
Мы посточнно допиливаем новые фичи, подключаем новые вендорские системы, что-то мы переделываем и оптимизируем.
Иногда экспериментируем с другими технологиями потоковой обработки, но не вместо Kafka, она вне конкуренции:)
Подумываем о том, чтобы прикрутить Machine Learning. В общем, куча идей и планов на будущее!
Было бы интересно услышать в комментариях, разрабатываете ли вы что-то подобное, например,
в области IoT с потоковой обработкой данных, и какие технологии применяете и как.
